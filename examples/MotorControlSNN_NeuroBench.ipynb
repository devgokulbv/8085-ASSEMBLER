{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devgokulbv/8085-ASSEMBLER/blob/main/examples/MotorControlSNN_NeuroBench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGgzXoJkyZiM"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBNKFLJIobYt"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"400\">](https://github.com/jeshraghian/snntorch/)\n",
        "\n",
        "# Regression with SNNs: Part I\n",
        "## Learning Membrane Potentials with LIF Neurons\n",
        "## By Alexander Henkes (https://orcid.org/0000-0003-4615-9271) and Jason K. Eshraghian (www.ncg.ucsc.edu)\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_regression_1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4blpfg4y44uO"
      },
      "source": [
        "This tutorial is based on the following papers on nonlinear regression and spiking neural networks. If you find these resources or code useful in your work, please consider citing the following sources:\n",
        "\n",
        "> <cite> [Alexander Henkes, Jason K. Eshraghian, and Henning Wessels. “Spiking neural networks for nonlinear regression\", arXiv preprint arXiv:2210.03515, October 2022.](https://arxiv.org/abs/2210.03515) </cite>\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnF_PEo5obYv",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In the regression tutorial series, you will learn how to use snnTorch to perform regression using a variety of spiking neuron models, including:\n",
        "\n",
        "* Leaky Integrate-and-Fire (LIF) Neurons\n",
        "* Recurrent LIF Neurons\n",
        "* Spiking LSTMs\n",
        "\n",
        "An overview of the regression tutorial series:\n",
        "\n",
        "* Part I (this tutorial) will train the membrane potential of a LIF neuron to follow a given trajectory over time.\n",
        "* Part II will use LIF neurons with recurrent feedback to perform classification using regression-based loss functions\n",
        "* Part III will use a more complex spiking LSTM network instead to train the firing time of a neuron.\n",
        "\n",
        "\n",
        "If running in Google Colab:\n",
        "* You may connect to GPU by checking `Runtime` > `Change runtime type` > `Hardware accelerator: GPU`\n",
        "* Next, install the latest PyPi distribution of snnTorch and Tonic by clicking into the following cell and pressing `Shift+Enter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6xe4AoIgyZik"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wE-J7Gi0yZip"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import statistics\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cufZenAyZir"
      },
      "source": [
        "Fix the random seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KJY63SGXyZiv"
      },
      "outputs": [],
      "source": [
        "# Seed\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peyaw6VvyZix"
      },
      "source": [
        "# 1. Spiking Regression\n",
        "## 1.1 A Quick Background on Linear and Nonlinear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGVWEEnyZiz"
      },
      "source": [
        "The tutorials so far have focused on multi-class classification problems. But if you've made it this far, then it's probably safe to say that your brain can do more than distinguish cats and dogs. You're amazing and we believe in you.\n",
        "\n",
        "An alternative problem is regression, where multiple input features $x_i$ are used to estimate an output on a continuous number line $y \\in \\mathbb{R}$.\n",
        "A classic example is estimating the price of a house, given a bunch of inputs such as land size, number of rooms, and the local demand for avocado toast.\n",
        "\n",
        "The objective of a regression problem is often the mean-square error:\n",
        "\n",
        "$$\\mathcal{L}_{MSE} = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2$$\n",
        "\n",
        "or the mean absolute error:\n",
        "\n",
        "$$\\mathcal{L}_{L1} = \\frac{1}{n}\\sum_{i=1}^n|y_i-\\hat{y_i}|$$\n",
        "\n",
        "\n",
        "where $y$ is the target and $\\hat{y}$ is the predicted value.\n",
        "\n",
        "One of the challenges of linear regression is that it can only use linear weightings of input features in predicting the output.\n",
        "Using a neural network trained using the mean-square error as the cost function allows us to perform nonlinear regression on more complex data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN-6Uhq6yZi0"
      },
      "source": [
        "## 1.2 Spiking Neurons in Regression\n",
        "\n",
        "Spikes are a type of nonlinearity that can also be used to learn more complex regression tasks.\n",
        "But if spiking neurons only emit spikes that are represented with 1's and 0's, then how might we perform regression? I'm glad you asked! Here are a few ideas:\n",
        "\n",
        "* Use the total number of spikes (a rate-based code)\n",
        "* Use the time of the spike (a temporal/latency-based code)\n",
        "* Use the distance between pairs of spikes (i.e., using the interspike interval)\n",
        "\n",
        "Or perhaps you pierce the neuron membrane with an electrical probe and decide to use the membrane potential instead, which is a continuous value.\n",
        "\n",
        "> Note: is it cheating to directly access the membrane potential, i.e., something that is meant to be a 'hidden state'? At this time, there isn't much consensus in the neuromorphic community. Despite being a high precision variable in many models (and thus computationally expensive), the membrane potential is commonly used in loss functions as it is a more 'continuous' variable compared to discrete time steps or spike counts. While it costs more in terms of power and latency to operate on higher-precision values, the impact might be minor if you have a small output layer, or if the output does not need to be scaled by weights. It really is a task-specific and hardware-specific question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLvhYqUfyZi1"
      },
      "source": [
        "# 2. Setting up the Regression Problem\n",
        "\n",
        "## 2.1 Create Dataset\n",
        "\n",
        "Let's construct a simple toy problem. The following class returns the function we are hoping to learn. If `mode = \"linear\"`, a straight line with a random slope is generated. If `mode = \"sqrt\"`, then the square root of this straight line is taken instead.\n",
        "\n",
        "Our goal: train a leaky integrate-and-fire neuron such that its membrane potential follows the sample over time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"Simple regression dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, labels):\n",
        "        \"\"\"Linear relation between input and output\"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of samples.\"\"\"\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"General implementation, but we only have one sample.\"\"\"\n",
        "        return self.dataset[idx, :], self.labels[idx, :]\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Get the input features and the label\n",
        "input_features = data.iloc[:, :12].values.astype(np.float32)\n",
        "label = data.iloc[:, 12].values.astype(np.float32)\n",
        "\n",
        "# Reshape the label to match the output shape of the model\n",
        "label = label.reshape((input_features.shape[0], 1))\n",
        "\n",
        "# Split the data into training, testing, and validation sets\n",
        "train_features, val_test_features, train_labels, val_test_labels = train_test_split(input_features, label, test_size=0.4, random_state=42)\n",
        "val_features, test_features, val_labels, test_labels = train_test_split(val_test_features, val_test_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create custom dataset instances for each set\n",
        "train_dataset = CustomDataset(train_features, train_labels)\n",
        "val_dataset = CustomDataset(val_features, val_labels)\n",
        "test_dataset = CustomDataset(test_features, test_labels)\n",
        "\n",
        "# Create DataLoader instances for each set\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True)\n"
      ],
      "metadata": {
        "id": "GoGyMiabSdNm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYT-mHT3yZi3"
      },
      "source": [
        "To see what a random sample looks like, run the following code-block:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWzT_KLyZi5"
      },
      "source": [
        "## 2.2 Create DataLoader\n",
        "\n",
        "The Dataset objects created above load data into memory, and the DataLoader will serve it up in batches. DataLoaders in PyTorch are a handy interface for passing data into a network. They return an iterator divided up into mini-batches of size ``batch_size``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyIKnkENyZi6"
      },
      "source": [
        "# 3. Construct Model\n",
        "\n",
        "Let us try a simple network using only leaky integrate-and-fire layers without recurrence.\n",
        "Subsequent tutorials will show how to use more complex neuron types with higher-order recurrence.\n",
        "These architectures should work just fine, if there is no strong time dependency in the data, i.e., the next time step has weak dependence on the previous one.\n",
        "\n",
        "A few notes on the architecture below:\n",
        "\n",
        "* Setting `learn_beta=True` enables the decay rate `beta` to be a learnable parameter\n",
        "* Each neuron has a unique, and randomly initialized threshold and decay rate\n",
        "* The output layer has the reset mechanism disabled by setting `reset_mechanism=\"none\"` as we will not use any output spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ByMjZOutyZi6"
      },
      "outputs": [],
      "source": [
        "# Define Network\n",
        "beta = 0.5\n",
        "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(12, 128)\n",
        "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad,reset_mechanism = \"zero\")\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad,reset_mechanism = \"none\")\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        cur1 = self.fc1(x)\n",
        "        spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "        cur2 = self.fc2(spk1)\n",
        "        spk2, mem2 = self.lif2(cur2, mem2)\n",
        "        return spk2, mem2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImfXEzZ6yZi7"
      },
      "source": [
        "Instantiate the network below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "k1eHFqsuyZi8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = Net().to(device)\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        module.bias.requires_grad = False\n",
        "        module.bias.data.zero_()\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        module.bias.requires_grad = False\n",
        "        module.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)\n",
        "print(model.fc2.bias)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5SGpHDT4erU",
        "outputId": "eeaef920-e5d9-418f-db77-7f1fcfe94935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0241,  0.0995,  0.0611,  ..., -0.2385,  0.0949, -0.1881],\n",
            "        [ 0.1831,  0.2587,  0.0393,  ..., -0.0693,  0.1149, -0.0537],\n",
            "        [-0.0585,  0.1746,  0.2587,  ..., -0.2689, -0.2539,  0.2186],\n",
            "        ...,\n",
            "        [ 0.0293,  0.2345, -0.2264,  ..., -0.0310,  0.1798, -0.0955],\n",
            "        [ 0.1634, -0.1926, -0.2371,  ...,  0.1720,  0.2023, -0.1321],\n",
            "        [ 0.0615,  0.2480, -0.2369,  ..., -0.2145,  0.2431, -0.2110]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "Parameter containing:\n",
            "tensor([0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbfoMYMWyZi9"
      },
      "source": [
        "Let's observe the behavior of the output neuron before it has been trained and how it compares to the target function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiV6JXH7yZjA"
      },
      "source": [
        "As the network has not yet been trained, it is unsurprising the membrane potential follows a senseless evolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9cyn5Y3yZjB"
      },
      "source": [
        "# 4. Construct Training Loop\n",
        "\n",
        "We call `torch.nn.MSELoss()` to minimize the mean square error between the membrane potential and the target evolution.\n",
        "\n",
        "We iterate over the same sample of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbiGhFXyZjC",
        "outputId": "2a557838-abbe-491c-fe56-9960a9f33b39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 1: Train - 1.878e-01, Val - 1.685e-01\n",
            "Loss after epoch 2: Train - 1.578e-01, Val - 1.450e-01\n",
            "Loss after epoch 3: Train - 1.404e-01, Val - 1.329e-01\n"
          ]
        }
      ],
      "source": [
        "num_iter = 10 # train for 10 epochs\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.MSELoss()\n",
        "\n",
        "loss_hist_train = [] # record loss for training set\n",
        "loss_hist_val = [] # record loss for validation set\n",
        "\n",
        "# training loop\n",
        "for epoch in range(num_iter):\n",
        "    train_batch = iter(train_dataloader)\n",
        "    val_batch = iter(val_dataloader)\n",
        "    minibatch_counter_train = 0\n",
        "    minibatch_counter_val = 0\n",
        "    loss_epoch_train = []\n",
        "    loss_epoch_val = []\n",
        "\n",
        "    # Training\n",
        "    for feature, label in train_batch:\n",
        "        # prepare data\n",
        "        feature = feature.to(device, dtype=torch.float32)  # Convert to Float32\n",
        "        label = label.to(device, dtype=torch.float32)  # Convert to Float32\n",
        "\n",
        "        # Run the model on the feature\n",
        "        output, mem = model(feature)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss_val = loss_function(mem, label)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate gradients\n",
        "        loss_val.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss\n",
        "        loss_hist_train.append(loss_val.item())\n",
        "        loss_epoch_train.append(loss_val.item())\n",
        "        minibatch_counter_train += 1\n",
        "\n",
        "    # Validation\n",
        "    model.eval() # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for feature, label in val_batch:\n",
        "            # prepare data\n",
        "            feature = feature.to(device, dtype=torch.float32)  # Convert to Float32\n",
        "            label = label.to(device, dtype=torch.float32)  # Convert to Float32\n",
        "\n",
        "            # Run the model on the feature\n",
        "            output, mem = model(feature)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss_val = loss_function(mem, label)\n",
        "\n",
        "            # Store loss\n",
        "            loss_hist_val.append(loss_val.item())\n",
        "            loss_epoch_val.append(loss_val.item())\n",
        "            minibatch_counter_val += 1\n",
        "\n",
        "    model.train() # set model back to training mode\n",
        "\n",
        "    avg_batch_loss_train = sum(loss_epoch_train) / minibatch_counter_train # calculate average loss p/epoch for training set\n",
        "    avg_batch_loss_val = sum(loss_epoch_val) / minibatch_counter_val # calculate average loss p/epoch for validation set\n",
        "    print(\"Loss after epoch {}: Train - %.3e, Val - %.3e\".format(epoch+1) % (avg_batch_loss_train, avg_batch_loss_val)) # print loss p/batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neurobench --quiet\n",
        "\n",
        "from neurobench.models import SNNTorchModel\n",
        "from neurobench.processors.postprocessors import ChooseMaxCount\n",
        "from neurobench.benchmarks import Benchmark\n",
        "from neurobench.metrics.workload import (\n",
        "    ActivationSparsity,\n",
        "    SynapticOperations,\n",
        "    #ClassificationAccuracy, # Remove this line\n",
        "    #MeanSquaredError, # Add this line for regression\n",
        ")\n",
        "from neurobench.metrics.static import (\n",
        "    Footprint,\n",
        "    ConnectionSparsity,\n",
        ")\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Create a SNNTorchModel instance\n",
        "# Wrap your model in a custom class to handle the input shape\n",
        "class CustomSNNTorchModel(SNNTorchModel):\n",
        "    def __call__(self, data):\n",
        "        # Reshape the input data to match your model's expected shape (batch_size, num_features)\n",
        "        data = data.squeeze(1)  # remove the timestep dimension\n",
        "        # Call the original forward method of your model\n",
        "        spk_out, mem_out = self.net(data)  # Get membrane potential\n",
        "\n",
        "        # Return membrane potential for regression metrics\n",
        "        #spikes = torch.stack([spk_out]).transpose(0, 1)\n",
        "        return mem_out  # Return membrane potential instead of spikes\n",
        "\n",
        "model1 = CustomSNNTorchModel(model)\n",
        "\n",
        "# Define preprocessors and postprocessors\n",
        "preprocessors = []\n",
        "#postprocessors = [ChooseMaxCount()]  # Consider removing or replacing for regression\n",
        "postprocessors = []  # Consider removing or replacing for regression\n",
        "\n",
        "# Define metrics\n",
        "static_metrics = [Footprint, ConnectionSparsity]\n",
        "workload_metrics = [ActivationSparsity, SynapticOperations]  # Use MeanSquaredError for regression\n",
        "\n",
        "# Create a Benchmark instance\n",
        "benchmark = Benchmark(model1, train_dataloader, preprocessors, postprocessors, [static_metrics, workload_metrics])\n",
        "\n",
        "# Run the benchmark\n",
        "results = benchmark.run(device=device)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "HNs9nSeFiYwd",
        "outputId": "ed41b454-ade0-4f75-f3c9-0d16bf3d627a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running benchmark\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 405/10874 [00:04<02:05, 83.19it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-a1951f59bbb1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Run the benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neurobench/benchmarks/benchmark.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, quiet, verbose, dataloader, preprocessors, postprocessors, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "vJ6waDFHtU45",
        "outputId": "bd939189-9c69-4d98-e53c-1ea2ca3af450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Footprint': 7728, 'ConnectionSparsity': 0.0, 'ActivationSparsity': 0.8223888259895945, 'SynapticOperations': {'Effective_MACs': 1536.0, 'Effective_ACs': 22.081661388858556, 'Dense': 1664.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input data\n",
        "test_features = test_features.reshape(-1, 12)\n",
        "\n",
        "# Create a custom dataset instance\n",
        "test_dataset = CustomDataset(test_features, test_labels)\n",
        "\n",
        "# Create a DataLoader instance\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, drop_last=True)\n",
        "\n",
        "# Create a SNNTorchModel instance\n",
        "model = SNNTorchModel(net)\n",
        "\n",
        "# Define preprocessors and postprocessors\n",
        "preprocessors = []\n",
        "postprocessors = [ChooseMaxCount()]\n",
        "\n",
        "# Define metrics\n",
        "static_metrics = [Footprint, ConnectionSparsity]\n",
        "workload_metrics = [ClassificationAccuracy, ActivationSparsity, SynapticOperations]\n",
        "\n",
        "# Create a Benchmark instance\n",
        "benchmark = Benchmark(model, test_dataloader, preprocessors, postprocessors, [static_metrics, workload_metrics])\n",
        "\n",
        "# Run the benchmark\n",
        "results = benchmark.run(device=\"cuda\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "NOhhvTCcbWYk",
        "outputId": "0ec89b0c-eabb-452f-c625-79978d6cc1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SNNTorchModel' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-39aecb6ec671>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create a SNNTorchModel instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNNTorchModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Define preprocessors and postprocessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neurobench/models/snntorch_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, net, custom_forward)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# add snntorch neuron layers as activation modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SNNTorchModel' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc2.weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "tvcrREbD4LG4",
        "outputId": "f4f64995-663d-4804-da86-5b04097b9b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-2.5043e-03, -1.1231e-02,  5.4316e-01,  ..., -1.6706e-01,\n",
            "          8.7534e-01, -4.5387e-01],\n",
            "        [ 1.7947e-01, -1.9556e-02,  1.0241e+00,  ...,  8.7854e-02,\n",
            "          2.9016e-01,  1.0045e+00],\n",
            "        [-3.8089e+00,  6.1794e-01, -3.7936e-01,  ...,  2.9451e-01,\n",
            "         -8.8330e-01,  2.0454e+00],\n",
            "        ...,\n",
            "        [-2.7458e+00,  2.8686e-01, -3.4865e+00,  ...,  4.0762e-02,\n",
            "         -3.8345e+00,  9.7601e-01],\n",
            "        [-6.2000e-01,  3.5391e-01, -1.4976e+00,  ..., -7.8688e-03,\n",
            "         -1.6293e+00,  1.2347e+00],\n",
            "        [ 1.1386e+00,  3.3705e-01,  8.0553e+00,  ...,  5.4486e-02,\n",
            "          8.0096e+00,  2.6376e+00]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0834,  0.0541,  0.6336,  0.0750, -0.2751, -0.0662,  0.2778,  0.2541,\n",
            "          0.0754,  0.0886,  0.2910, -0.1417,  0.1975,  0.1577,  0.2380, -0.1074,\n",
            "         -0.9951,  0.1862,  0.0925,  0.4374,  0.4182,  0.1217,  0.0367, -0.0824,\n",
            "          0.0926,  0.1512, -0.6263, -0.0532,  0.1667, -0.0659,  0.0917, -0.1423,\n",
            "          0.0747,  0.0774,  0.0924,  0.0550,  0.1785,  0.0378,  0.2976,  0.2692,\n",
            "          0.2745, -0.1004, -0.1353,  0.2251,  0.2613, -0.0856,  0.2016,  0.1021,\n",
            "          0.3878, -0.7680, -0.0538, -0.0892, -0.2858,  0.0031,  0.6813, -0.1235,\n",
            "         -0.2743, -0.0935,  0.2465,  0.2413,  0.3579,  0.3567, -0.0377, -0.1043,\n",
            "         -0.2039,  0.2088,  0.5012,  0.5624, -0.0483,  0.2055,  0.2421,  0.7846,\n",
            "         -0.0923, -0.3683,  0.2238, -0.2167, -0.1869,  0.1026, -0.1546, -0.0148,\n",
            "         -0.0858,  0.0597, -0.0829,  0.2570,  0.0333,  0.2701,  0.2580, -0.0448,\n",
            "          0.0709, -0.0785,  0.0177, -0.0966,  0.0771,  0.1187,  0.2434,  0.4214,\n",
            "         -0.0402,  0.1434,  0.2436,  0.2515,  0.2356,  0.0704,  0.1663, -0.0824,\n",
            "         -0.0022, -0.0967,  0.1467, -0.0714, -0.0810, -0.0762,  0.1536, -0.1464,\n",
            "          0.2322,  0.1405, -0.0594, -0.0172, -0.5425, -0.3311,  0.2232,  0.0368,\n",
            "          0.0886, -0.0997,  0.0651, -0.2392,  0.0590, -0.8387, -0.1592,  0.6261]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Net' object has no attribute 'fc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-3723efcbd572>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Net' object has no attribute 'fc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnpHG9b-yZjD"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPWLbIDeyZjD",
        "outputId": "167dbaca-8db8-4841-8b94-08c4ae59b371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: tensor([[3.5382],\n",
            "        [3.5566],\n",
            "        [3.5750],\n",
            "        [3.5873],\n",
            "        [3.6073],\n",
            "        [3.6257],\n",
            "        [3.6441],\n",
            "        [3.6625],\n",
            "        [3.6763],\n",
            "        [3.6947],\n",
            "        [3.7131],\n",
            "        [3.7331],\n",
            "        [3.7454],\n",
            "        [3.7638],\n",
            "        [3.7822],\n",
            "        [3.8021],\n",
            "        [3.8144],\n",
            "        [3.8328],\n",
            "        [3.8512],\n",
            "        [3.8712],\n",
            "        [3.8835],\n",
            "        [3.9019],\n",
            "        [3.9203],\n",
            "        [3.9402],\n",
            "        [3.9525],\n",
            "        [3.9709],\n",
            "        [3.9893],\n",
            "        [4.0093],\n",
            "        [4.0215],\n",
            "        [4.0400],\n",
            "        [4.0599],\n",
            "        [4.0783]], device='cuda:0', dtype=torch.float64)\n",
            "Prediction: tensor([[3.7567],\n",
            "        [3.7567],\n",
            "        [3.7527],\n",
            "        [3.7527],\n",
            "        [3.7527],\n",
            "        [3.7767],\n",
            "        [3.7767],\n",
            "        [3.5712],\n",
            "        [4.0725],\n",
            "        [4.3700],\n",
            "        [4.3700],\n",
            "        [4.3700],\n",
            "        [4.3700],\n",
            "        [4.6121],\n",
            "        [4.6121],\n",
            "        [4.3495],\n",
            "        [4.3495],\n",
            "        [4.4266],\n",
            "        [4.4266],\n",
            "        [4.4266],\n",
            "        [4.4266],\n",
            "        [4.4820],\n",
            "        [4.4820],\n",
            "        [4.4820],\n",
            "        [4.4820],\n",
            "        [4.4820],\n",
            "        [4.5524],\n",
            "        [4.5524],\n",
            "        [4.3112],\n",
            "        [4.3112],\n",
            "        [4.6642],\n",
            "        [4.6642]], device='cuda:0')\n",
            "\n",
            "Label: tensor([[4.0906],\n",
            "        [4.1090],\n",
            "        [4.1290],\n",
            "        [4.1474],\n",
            "        [4.1596],\n",
            "        [4.1780],\n",
            "        [4.1980],\n",
            "        [4.2164],\n",
            "        [4.2348],\n",
            "        [4.2471],\n",
            "        [4.2670],\n",
            "        [4.2855],\n",
            "        [4.3039],\n",
            "        [4.3161],\n",
            "        [4.3361],\n",
            "        [4.3545],\n",
            "        [4.3729],\n",
            "        [4.3867],\n",
            "        [4.4051],\n",
            "        [4.4235],\n",
            "        [4.4420],\n",
            "        [4.4558],\n",
            "        [4.4742],\n",
            "        [4.4926],\n",
            "        [4.5110],\n",
            "        [4.5248],\n",
            "        [4.5432],\n",
            "        [4.5616],\n",
            "        [4.5801],\n",
            "        [4.5939],\n",
            "        [4.6123],\n",
            "        [4.6307]], device='cuda:0', dtype=torch.float64)\n",
            "Prediction: tensor([[4.6642],\n",
            "        [4.6642],\n",
            "        [4.6642],\n",
            "        [4.6642],\n",
            "        [4.6642],\n",
            "        [4.4207],\n",
            "        [4.4540],\n",
            "        [4.4540],\n",
            "        [4.7005],\n",
            "        [4.7005],\n",
            "        [4.7036],\n",
            "        [4.7036],\n",
            "        [4.7036],\n",
            "        [4.7633],\n",
            "        [4.5382],\n",
            "        [4.5382],\n",
            "        [4.7045],\n",
            "        [4.7045],\n",
            "        [4.7045],\n",
            "        [4.7045],\n",
            "        [4.9277],\n",
            "        [4.9277],\n",
            "        [4.9277],\n",
            "        [4.9277],\n",
            "        [4.9277],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736]], device='cuda:0')\n",
            "\n",
            "Label: tensor([[4.6506],\n",
            "        [4.6629],\n",
            "        [4.6813],\n",
            "        [4.6997],\n",
            "        [4.7197],\n",
            "        [4.7320],\n",
            "        [4.7504],\n",
            "        [4.7688],\n",
            "        [4.7887],\n",
            "        [4.8010],\n",
            "        [4.8194],\n",
            "        [4.8378],\n",
            "        [4.8578],\n",
            "        [4.8700],\n",
            "        [4.8885],\n",
            "        [4.9069],\n",
            "        [4.9268],\n",
            "        [4.9391],\n",
            "        [4.9575],\n",
            "        [4.9774],\n",
            "        [4.9959],\n",
            "        [5.0081],\n",
            "        [5.0265],\n",
            "        [5.0465],\n",
            "        [5.0649],\n",
            "        [5.0772],\n",
            "        [5.0956],\n",
            "        [5.1155],\n",
            "        [5.1340],\n",
            "        [5.1462],\n",
            "        [5.1646],\n",
            "        [5.1846]], device='cuda:0', dtype=torch.float64)\n",
            "Prediction: tensor([[4.8736],\n",
            "        [4.8736],\n",
            "        [4.8736],\n",
            "        [5.0599],\n",
            "        [5.0599],\n",
            "        [5.0599],\n",
            "        [5.0599],\n",
            "        [5.1140],\n",
            "        [5.1140],\n",
            "        [5.1140],\n",
            "        [4.9706],\n",
            "        [5.0244],\n",
            "        [5.0244],\n",
            "        [5.0244],\n",
            "        [5.0244],\n",
            "        [5.0244],\n",
            "        [5.1441],\n",
            "        [5.3109],\n",
            "        [5.3109],\n",
            "        [5.3109],\n",
            "        [5.3486],\n",
            "        [5.3486],\n",
            "        [5.3338],\n",
            "        [5.3338],\n",
            "        [5.3338],\n",
            "        [5.2151],\n",
            "        [5.2151],\n",
            "        [5.2528],\n",
            "        [5.4908],\n",
            "        [5.4908],\n",
            "        [5.4908],\n",
            "        [5.4908]], device='cuda:0')\n",
            "\n",
            "Label: tensor([[5.2030],\n",
            "        [5.2153],\n",
            "        [5.2337],\n",
            "        [5.2536],\n",
            "        [5.2720],\n",
            "        [5.2843],\n",
            "        [5.3043],\n",
            "        [5.3227],\n",
            "        [5.3411],\n",
            "        [5.3534],\n",
            "        [5.3733],\n",
            "        [5.3917],\n",
            "        [5.4101],\n",
            "        [5.4224],\n",
            "        [5.4424],\n",
            "        [5.4608],\n",
            "        [5.4792],\n",
            "        [5.4915],\n",
            "        [5.5114],\n",
            "        [5.5298],\n",
            "        [5.5482],\n",
            "        [5.5605],\n",
            "        [5.5805],\n",
            "        [5.5989],\n",
            "        [5.6173],\n",
            "        [5.6311],\n",
            "        [5.6495],\n",
            "        [5.6679],\n",
            "        [5.6863],\n",
            "        [5.7001],\n",
            "        [5.7185],\n",
            "        [5.7370]], device='cuda:0', dtype=torch.float64)\n",
            "Prediction: tensor([[5.4908],\n",
            "        [5.5459],\n",
            "        [5.4537],\n",
            "        [5.4537],\n",
            "        [5.4089],\n",
            "        [5.4089],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.4913],\n",
            "        [5.5910],\n",
            "        [5.5910],\n",
            "        [5.7455],\n",
            "        [5.9889],\n",
            "        [5.9889],\n",
            "        [5.9889],\n",
            "        [6.1759],\n",
            "        [6.1759]], device='cuda:0')\n",
            "\n",
            "Label: tensor([[5.7554],\n",
            "        [5.7692],\n",
            "        [5.7876],\n",
            "        [5.8060],\n",
            "        [5.8244],\n",
            "        [5.8382],\n",
            "        [5.8566],\n",
            "        [5.8750],\n",
            "        [5.8950],\n",
            "        [5.9073],\n",
            "        [5.9257],\n",
            "        [5.9441],\n",
            "        [5.9579],\n",
            "        [5.9763],\n",
            "        [5.9947],\n",
            "        [6.0131],\n",
            "        [6.0269],\n",
            "        [6.0454],\n",
            "        [6.0638],\n",
            "        [6.0822],\n",
            "        [6.0960],\n",
            "        [6.1144],\n",
            "        [6.1328],\n",
            "        [6.1512],\n",
            "        [6.1650],\n",
            "        [6.1835],\n",
            "        [6.2019],\n",
            "        [6.2218],\n",
            "        [6.2341],\n",
            "        [6.2525],\n",
            "        [6.2709],\n",
            "        [0.0000]], device='cuda:0', dtype=torch.float64)\n",
            "Prediction: tensor([[6.1759],\n",
            "        [6.1759],\n",
            "        [6.1759],\n",
            "        [6.3016],\n",
            "        [6.3016],\n",
            "        [6.2475],\n",
            "        [6.2475],\n",
            "        [6.2475],\n",
            "        [6.2475],\n",
            "        [6.2475],\n",
            "        [6.0874],\n",
            "        [6.3304],\n",
            "        [6.3304],\n",
            "        [6.3304],\n",
            "        [6.3304],\n",
            "        [6.2555],\n",
            "        [6.2555],\n",
            "        [6.1588],\n",
            "        [6.5069],\n",
            "        [6.7461],\n",
            "        [6.7461],\n",
            "        [6.5106],\n",
            "        [6.3683],\n",
            "        [6.2219],\n",
            "        [6.9815],\n",
            "        [6.9815],\n",
            "        [6.5478],\n",
            "        [6.5478],\n",
            "        [7.1814],\n",
            "        [5.9291],\n",
            "        [6.7136],\n",
            "        [6.3569]], device='cuda:0')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_batch = iter(dataloader)\n",
        "\n",
        "# run a few forward-passes\n",
        "with torch.no_grad():\n",
        "    for _ in range(5):\n",
        "        batch = next(train_batch)\n",
        "        feature, label = batch\n",
        "        feature = feature.to(device, dtype=torch.float32)  # Convert to Float32\n",
        "        label = label.to(device)\n",
        "        output, mem = model(feature)\n",
        "        print(\"Label:\", label)\n",
        "        print(\"Prediction:\", mem)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a numpy array with the 12 values\n",
        "input_values = np.array([\n",
        "    0.400165915, -1.274127364, 0.057144079, -0.1130144,\n",
        "    0.431862235, -1.264977455, 0.060369313, -0.111537278,\n",
        "    0.459596455, -1.244390249, 0.059304267, -0.111418493\n",
        "]).reshape(1, 12)\n",
        "\n",
        "# Make a prediction with the model\n",
        "input_values = torch.tensor(input_values, dtype=torch.float32).to(device)\n",
        "output, mem = model(input_values)\n",
        "\n",
        "# Print the prediction\n",
        "print(\"Prediction:\", mem.item())\n"
      ],
      "metadata": {
        "id": "Yh-uZ-pZ_g5r",
        "outputId": "e320260c-3ea8-452f-d4a5-4b9ffb9fee0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 3.7566916942596436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights of the model\n",
        "layer1_weights = model.fc1.weight.detach().cpu().numpy().flatten()\n",
        "layer2_weights = model.fc2.weight.detach().cpu().numpy().flatten()\n",
        "\n",
        "# Store the weights in a text file\n",
        "with open(\"weights.txt\", \"w\") as f:\n",
        "    f.write(\"const float weights_layer1[{}] = {{\\n\".format(len(layer1_weights)))\n",
        "    for i in range(len(layer1_weights)):\n",
        "        f.write(\"  {:.6f}\".format(layer1_weights[i]))\n",
        "        if i < len(layer1_weights) - 1:\n",
        "            f.write(\",\")\n",
        "       # f.write(\"\\n\")\n",
        "    f.write(\"};\\n\")\n",
        "\n",
        "    f.write(\"const float weights_layer2[{}] = {{\\n\".format(len(layer2_weights)))\n",
        "    for i in range(len(layer2_weights)):\n",
        "        f.write(\"  {:.6f}\".format(layer2_weights[i]))\n",
        "        if i < len(layer2_weights) - 1:\n",
        "            f.write(\",\")\n",
        "       # f.write(\"\\n\")\n",
        "    f.write(\"};\\n\")\n"
      ],
      "metadata": {
        "id": "YXmA3i9e_69P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTVkbZdFyZjE"
      },
      "source": [
        "Let's plot our results for some visual intuition:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVOLe0fWyZjF"
      },
      "source": [
        "It is a little jagged, but it's not looking too bad.\n",
        "\n",
        "You might try to improve the curve fit by expanding the size of the hidden layer, increasing the number of iterations, adding extra time steps, hyperparameter fine-tuning, or using a completely different neuron type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmIK2fqcyZjG"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "The next regression tutorials will test more powerful spiking neurons, such as Recurrent LIF neurons and spiking LSTMs, to see how they compare.\n",
        "\n",
        "If you like this project, please consider starring ⭐ the repo on GitHub as it is the easiest and best way to support it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_3CodSyZjH"
      },
      "source": [
        "# Additional Resources\n",
        "* [Check out the snnTorch GitHub project here.](https://github.com/jeshraghian/snntorch)\n",
        "* More detail on nonlinear regression with SNNs can be found in our corresponding preprint here: [Henkes, A.; Eshraghian, J. K.; and Wessels, H.  “Spiking neural networks for nonlinear regression\", arXiv preprint arXiv:2210.03515, Oct. 2022.](https://arxiv.org/abs/2210.03515)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "collapsed_sections": [
        "9QXsrr6Mp5e_",
        "1EWDw3bip8Ie",
        "vFM8UV9CreIX",
        "xXkTAJ9ws1Y6",
        "OgkWg605tE1y",
        "OBt0WDzyujnk",
        "xC96eesMqYo-",
        "mszPTrYOluym",
        "VTHK-wAWV57B"
      ],
      "name": "snntorch_regression_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "2a3056c17c3c31a88ffeb08a28ff32bf922ba3f6fa0343ca62cc95241e30c809"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}